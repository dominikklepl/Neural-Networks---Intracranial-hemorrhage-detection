{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_CNN from scratch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominikklepl/Neural-Networks-Intracranial-hemorrhage-detection/blob/master/04_CNN_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCqsoxkxP9zB",
        "colab_type": "text"
      },
      "source": [
        "# Building CNN from scratch\n",
        "In previous notebook, we built only the classifier part of the model and used pretrainted convolutional layers for automatic feature extraction.\n",
        "\n",
        "We'll just copy paste functions for preprocessing and data generation from previous notebook (\"03 Transfer learning\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVc8B0wP_49g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prototyping = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc1NeHJZVUfI",
        "colab_type": "text"
      },
      "source": [
        "## Setup and paths to data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5ieuRPWhad7",
        "colab_type": "text"
      },
      "source": [
        "Install missing packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgSMnAEchcQC",
        "colab_type": "code",
        "outputId": "fa7bf591-8fa1-4788-ff72-e138ee7a8e6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install pydicom"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/d5/da1fdf3b967e324ee47a7ad9553c9b94c1193b6b98afd9eeda0efb76b9f7/pydicom-1.3.0-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHyl7ejJRHbp",
        "colab_type": "text"
      },
      "source": [
        "Import all required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iC6GizdRLOe",
        "colab_type": "code",
        "outputId": "9872fd55-6870-473e-82ee-6a0c367a9616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#connect google drive\n",
        "from google.colab import drive\n",
        "\n",
        "#dealing with zip\n",
        "import zipfile\n",
        "\n",
        "#importing labels and working with dataframe\n",
        "import pandas as pd\n",
        "\n",
        "#manipulation with images\n",
        "import pydicom\n",
        "import torch\n",
        "import numpy as np\n",
        "from math import ceil, floor, log\n",
        "import cv2\n",
        "\n",
        "#train-test\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "#models\n",
        "%tensorflow_version 1.x\n",
        "import keras.utils\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Activation, concatenate, Dropout\n",
        "from keras.initializers import glorot_normal, he_normal\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Model, load_model\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.nn import sigmoid_cross_entropy_with_logits\n",
        "\n",
        "#save model architecture\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "#pretrained models\n",
        "from keras_applications.resnet import ResNet50\n",
        "from keras_applications.inception_v3 import InceptionV3\n",
        "\n",
        "#plotting\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ5jhXN5R4uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ignore deprecation warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wneWaNJIxllY",
        "colab_type": "text"
      },
      "source": [
        "Check that GPU is available and that keras can use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGOK4bYJxn6J",
        "colab_type": "code",
        "outputId": "2ff63a25-f17e-4030-cac2-73605d6939b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/job:localhost/replica:0/task:0/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNASn3sSluA-",
        "colab_type": "text"
      },
      "source": [
        "Set random seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsH2A1Qhlxep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(31415)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_9xRxp2XYhR",
        "colab_type": "text"
      },
      "source": [
        "Connect Google Drive. That's where my data is stored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAqMR_oS_3De",
        "colab_type": "code",
        "outputId": "6204a90d-65be-4eb3-aa51-a2d109e4e9b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "GDRIVE_PATH = \"/gdrive\"\n",
        "drive.mount(GDRIVE_PATH)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KyezdSiRqxy",
        "colab_type": "text"
      },
      "source": [
        "Set paths to all required data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mwN-s-7Rqeq",
        "colab_type": "code",
        "outputId": "7d00db85-bab3-45b3-a4b8-2c5ff68394e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "WORK_DIR = \"/content/\"\n",
        "BASE_DIR = GDRIVE_PATH + \"/My Drive/\"\n",
        "ZIP_PATH = BASE_DIR + \"DICOMS/train_images.zip\"\n",
        "DF_PATH = BASE_DIR + \"train_balanced.csv\"\n",
        "MODEL_PATH = BASE_DIR + \"models/\"\n",
        "RESULT_PATH = BASE_DIR + \"results/\" #for saving performance of models\n",
        "IMAGES_PATH = WORK_DIR + \"img\"\n",
        "\n",
        "#if the model and results aren't created already, create them\n",
        "!mkdir /gdrive/My\\ Drive/models\n",
        "!mkdir /gdrive/My\\ Drive/results\n",
        "\n",
        "#also create temporary folder in working directory for unzipping images\n",
        "!mkdir /content/img"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/gdrive/My Drive/models’: File exists\n",
            "mkdir: cannot create directory ‘/gdrive/My Drive/results’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2wpHxQE7Yta",
        "colab_type": "code",
        "outputId": "e2c7a180-854d-4caf-ca9e-b2fcbc34d6fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#get number of CPU cores\n",
        "import multiprocessing\n",
        "CORES = multiprocessing.cpu_count()\n",
        "print(CORES)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUE6PFH6Va7f",
        "colab_type": "text"
      },
      "source": [
        "Images are saved in a zip file. For easier and faster manipulation, let's extract them to a folder in the working directory that we created before (\"/content/img\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDrNJugxVZAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_archive = zipfile.ZipFile(ZIP_PATH)\n",
        "%time img_archive.extractall(path=IMAGES_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QwB9UwVXyyk",
        "colab_type": "text"
      },
      "source": [
        "### Load the csv with labels and metadata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UrkCTAJXyZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv(DF_PATH)\n",
        "train_df.sample(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYdHHbORadqN",
        "colab_type": "text"
      },
      "source": [
        "Some of the columns are useless at this point, let's keep just those that we actually need. Also some of the columns have too long name, yes SOPInstanceUID I'm talking about you so we'll rename them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqW9VJAoamXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = train_df[['SOPInstanceUID',\n",
        "                  'PatientID',\n",
        "                  'any',\n",
        "                  'epidural',\n",
        "                  'intraparenchymal',\n",
        "                  'intraventricular',\n",
        "                  'subarachnoid',\n",
        "                  'subdural']].copy()\n",
        "train.rename(columns={'SOPInstanceUID': 'ID',\n",
        "                      'PatientID': 'Patient'},\n",
        "              inplace=True)\n",
        "#shuffle rows\n",
        "train = train.sample(frac=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbkJiWhT_seU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if prototyping:\n",
        "  train = train.sample(200)\n",
        "  train = train.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFws-IEliua2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSi-wdBbxWkr",
        "colab_type": "text"
      },
      "source": [
        "### Load bins for uniform transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcHBPvr7xWDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bins = torch.load(BASE_DIR+\"bins.pt\").numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctj4nBrbThpT",
        "colab_type": "text"
      },
      "source": [
        "## Data processing functions\n",
        "Before we can start building awesome neural networks, we need a few helper functions for constructing data (i.e. images) in form that's acceptable for the models to learn from.\n",
        "\n",
        "First, functions for reading DICOM and applying intercept and slope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzN-9sVgeYny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#correcting dcm with wrong meta-data (explained in preprocessing notebook)\n",
        "def correct_dcm(dcm):\n",
        "    x = dcm.pixel_array + 1000\n",
        "    px_mode = 4096\n",
        "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
        "    dcm.PixelData = x.tobytes()\n",
        "    dcm.RescaleIntercept = -1000\n",
        "\n",
        "#transform dicom to HU by applying intercept and slope\n",
        "def rescale_dcm(dcm):\n",
        "  if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
        "    correct_dcm(dcm)\n",
        "  return dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s6f3kDaw_At",
        "colab_type": "text"
      },
      "source": [
        "### Use full scale but apply uniform distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0FPSzjDqVbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_full(dcm, bins):\n",
        "  ys = np.linspace(0., 1., len(bins))\n",
        "  x = dcm.flatten()\n",
        "  x = np.interp(x, bins, ys)\n",
        "  x = x.reshape(dcm.shape).clip(0.,1.) * 2**16\n",
        "  x = x.astype(np.uint16)\n",
        "  x[x<np.median(x)]=0\n",
        "  x = np.stack((x,)*3, axis=-1)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyrYCyH6t3HE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test it\n",
        "dcm = pydicom.dcmread(IMAGES_PATH+\"/\"+train.ID[0]+\".dcm\")\n",
        "dcm = rescale_dcm(dcm)\n",
        "x = to_full(dcm, bins)\n",
        "plt.imshow(x[:,:,0], cmap=plt.cm.bone)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24YQ6mt2gsRT",
        "colab_type": "text"
      },
      "source": [
        "### Getting channels and save them as color channels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZsYW4pbgxb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def window_image(img, window_center, window_width, U=1.0, eps=(1.0 / 255.0)):\n",
        "    ue = np.log((U / eps) - 1.0)\n",
        "    W = (2 / window_width) * ue\n",
        "    b = ((-2 * window_center) / window_width) * ue\n",
        "    z = W * img + b\n",
        "    img = U / (1 + np.power(np.e, -1.0 * z))\n",
        "    img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
        "    return img\n",
        "\n",
        "def to_channels(dcm):\n",
        "    brain_img = window_image(dcm, 40, 80)\n",
        "    subdural_img = window_image(dcm, 80, 200)\n",
        "    soft_img = window_image(dcm, 40, 380)\n",
        "    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n",
        "    return bsb_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgUSlI3hUCzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Finally function for loading a single image and resizing it to the input size.\n",
        "def read_img(ID, resize, preprocess):\n",
        "    path = IMAGES_PATH+'/'+ID+'.dcm'\n",
        "    dcm = pydicom.dcmread(path)\n",
        "    dcm = rescale_dcm(dcm)\n",
        "\n",
        "    if preprocess is \"CH\":\n",
        "      try: img = to_channels(dcm)\n",
        "      except: img = np.zeros(resize)\n",
        "    if preprocess is \"F\":\n",
        "      try: img = to_full(dcm, bins)\n",
        "      except: img = np.zeros(resize)\n",
        "\n",
        "    img = cv2.resize(img, resize[:2], interpolation=cv2.INTER_CUBIC) #resize\n",
        "    img = (img - np.min(img))/(np.max(img)-np.min(img)) #min-max normalize\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDYDdmsF5Vv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leZ6duPAfTQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%time img_ch = read_img(train.ID[2], (250, 250, 3), preprocess=\"CH\")\n",
        "%time img_f = read_img(train.ID[2], (250, 250, 3), preprocess=\"F\")\n",
        "\n",
        "#some sanity checks\n",
        "plt.imshow(img_ch)\n",
        "print(img_ch.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXSwicCIzbE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(img_f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRXDvpH0hUB0",
        "colab_type": "text"
      },
      "source": [
        "### Data generator\n",
        "This function prepares each batch for feeding to neural network. First, it gets random IDs and associated labels. Then shuffles them, to reduce bias. Finally, the images, using the IDs are loaded using the _read function defined above.\n",
        "\n",
        "The function is generalized for both training and testing data.\n",
        "\n",
        "It is derived from keras.sequence so it should be enabled for multiprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lazYV0FW5VDi",
        "colab_type": "text"
      },
      "source": [
        "#### Full-preprocessing DataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN15CiG35bEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGenerator_Full(keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, IDs, labels=None, batch_size=30, img_size=(299, 299, 3), num_classes=5, *args, **kwargs):\n",
        "        self.IDs = IDs\n",
        "        self.labels = labels.loc[:, 'any':'subdural']\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.num_classes = num_classes\n",
        "        self.on_epoch_end()\n",
        "\n",
        "#define number of steps per epoch\n",
        "    def __len__(self):\n",
        "        return int(ceil(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        IDs_batch = self.IDs[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        if self.labels is not None:\n",
        "          X, y_any, y_subtype = self.__data_generation(IDs_batch)\n",
        "          return X, [y_any, y_subtype]\n",
        "        else:\n",
        "            X = self.__data_generation(IDs_batch)\n",
        "            return X\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        if self.labels is not None: #during training we shuffle the images\n",
        "            self.indices = np.arange(len(self.IDs))\n",
        "            np.random.shuffle(self.indices)\n",
        "        else:\n",
        "            self.indices = np.arange(len(self.IDs))\n",
        "\n",
        "    def __data_generation(self, IDs_batch):\n",
        "        X = np.empty((self.batch_size, *self.img_size))\n",
        "        if self.labels is not None: # training\n",
        "          y_subtype = np.empty((self.batch_size, self.num_classes), dtype=np.float32)\n",
        "          y_any = np.empty((self.batch_size, 1), dtype=np.float32)\n",
        "          for i, ID in enumerate(IDs_batch):\n",
        "            X[i,] = read_img(ID, self.img_size, preprocess=\"F\")\n",
        "            y_any[i], y_subtype[i] = self.__get_target(ID)\n",
        "            return X, y_any, y_subtype\n",
        "        else: # testing\n",
        "            for i, ID in enumerate(IDs_batch):\n",
        "                X[i,] = read_img(ID, self.img_size, preprocess=\"F\")\n",
        "            return X\n",
        "\n",
        "    def __get_target(self, ID):\n",
        "        y_any = self.labels.loc[ID, \"any\"]\n",
        "        y_subtype = self.labels.drop(\"any\", axis=1).loc[ID].values\n",
        "        return y_any, y_subtype"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eVlpe8H5bqQ",
        "colab_type": "text"
      },
      "source": [
        "#### Channels-preprocessing DataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wEHR1byhydD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGenerator_Channels(keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, IDs, labels=None, batch_size=30, img_size=(299, 299, 3), num_classes=5, *args, **kwargs):\n",
        "        self.IDs = IDs\n",
        "        self.labels = labels.loc[:, 'any':'subdural']\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.num_classes = num_classes\n",
        "        self.on_epoch_end()\n",
        "\n",
        "#define number of steps per epoch\n",
        "    def __len__(self):\n",
        "        return int(ceil(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        IDs_batch = self.IDs[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        if self.labels is not None:\n",
        "          X, y_any, y_subtype = self.__data_generation(IDs_batch)\n",
        "          return X, [y_any, y_subtype]\n",
        "        else:\n",
        "            X = self.__data_generation(IDs_batch)\n",
        "            return X\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        if self.labels is not None: #during training we shuffle the images\n",
        "            self.indices = np.arange(len(self.IDs))\n",
        "            np.random.shuffle(self.indices)\n",
        "        else:\n",
        "            self.indices = np.arange(len(self.IDs))\n",
        "\n",
        "    def __data_generation(self, IDs_batch):\n",
        "        X = np.empty((self.batch_size, *self.img_size))\n",
        "        if self.labels is not None: # training\n",
        "          y_subtype = np.empty((self.batch_size, self.num_classes), dtype=np.float32)\n",
        "          y_any = np.empty((self.batch_size, 1), dtype=np.float32)\n",
        "          for i, ID in enumerate(IDs_batch):\n",
        "            X[i,] = read_img(ID, self.img_size, preprocess=\"CH\")\n",
        "            y_any[i], y_subtype[i] = self.__get_target(ID)\n",
        "            return X, y_any, y_subtype\n",
        "        else: # testing\n",
        "            for i, ID in enumerate(IDs_batch):\n",
        "                X[i,] = read_img(ID, self.img_size, preprocess=\"CH\")\n",
        "            return X\n",
        "\n",
        "    def __get_target(self, ID):\n",
        "        y_any = self.labels.loc[ID, \"any\"]\n",
        "        y_subtype = self.labels.drop(\"any\", axis=1).loc[ID].values\n",
        "        return y_any, y_subtype"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi13nw0AyhWM",
        "colab_type": "text"
      },
      "source": [
        "## Loss functions and performance metrics\n",
        "\n",
        "### Multilabel Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOlxhFDIymLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def np_multilabel_loss(y_true, y_pred, class_weights=None):\n",
        "    y_pred = np.where(y_pred > 1-(1e-07), 1-1e-07, y_pred)\n",
        "    y_pred = np.where(y_pred < 1e-07, 1e-07, y_pred)\n",
        "    single_class_cross_entropies = - np.mean(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred), axis=0)\n",
        "    \n",
        "    print(single_class_cross_entropies)\n",
        "    if class_weights is None:\n",
        "        loss = np.mean(single_class_cross_entropies)\n",
        "    else:\n",
        "        loss = np.sum(class_weights*single_class_cross_entropies)\n",
        "    return loss\n",
        "\n",
        "def get_raw_xentropies(y_true, y_pred):\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1-1e-7)\n",
        "    xentropies = y_true * tf.log(y_pred) + (1-y_true) * tf.log(1-y_pred)\n",
        "    return -xentropies\n",
        "\n",
        "def multilabel_focal_loss(class_weights=None, alpha=0.5, gamma=2):\n",
        "    def mutlilabel_focal_loss_inner(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.cast(y_pred, tf.float32)\n",
        "        \n",
        "        xentropies = get_raw_xentropies(y_true, y_pred)\n",
        "\n",
        "        # compute pred_t:\n",
        "        y_t = tf.where(tf.equal(y_true,1), y_pred, 1.-y_pred)\n",
        "        alpha_t = tf.where(tf.equal(y_true, 1), alpha * tf.ones_like(y_true), (1-alpha) * tf.ones_like(y_true))\n",
        "\n",
        "        # compute focal loss contributions\n",
        "        focal_loss_contributions =  tf.multiply(tf.multiply(tf.pow(1-y_t, gamma), xentropies), alpha_t) \n",
        "\n",
        "        # our focal loss contributions have shape (n_samples, s_classes), we need to reduce with mean over samples:\n",
        "        focal_loss_per_class = tf.reduce_mean(focal_loss_contributions, axis=0)\n",
        "\n",
        "        # compute the overall loss if class weights are None (equally weighted):\n",
        "        if class_weights is None:\n",
        "            focal_loss_result = tf.reduce_mean(focal_loss_per_class)\n",
        "        else:\n",
        "            # weight the single class losses and compute the overall loss\n",
        "            weights = tf.constant(class_weights, dtype=tf.float32)\n",
        "            focal_loss_result = tf.reduce_sum(tf.multiply(weights, focal_loss_per_class))\n",
        "            \n",
        "        return focal_loss_result\n",
        "    return mutlilabel_focal_loss_inner\n",
        "\n",
        "loss_f = multilabel_focal_loss([0.1, 0.1, 0.1, 0.1, 0.1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV4yoVE92WKx",
        "colab_type": "text"
      },
      "source": [
        "### Performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtFeTEcf2atK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#F1 score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQZ78rNo2fPJ",
        "colab_type": "text"
      },
      "source": [
        "## Transfer learning model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuhfqQBAAv-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transfer_network_F:\n",
        "    \n",
        "    def __init__(self, engine, loss_fun, metrics_list, input_dims, train_generator, val_generator,\n",
        "                 epochs, learning_rate=1e-3, num_classes=5, checkpoint_path=MODEL_F_PATH, weights='imagenet'):\n",
        "        \n",
        "        self.engine = engine\n",
        "        self.loss_fun = loss_fun\n",
        "        self.metrics_list = metrics_list\n",
        "        self.input_dims = input_dims\n",
        "        self.train_generator = train_generator\n",
        "        self.val_generator = val_generator\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_classes = num_classes\n",
        "        self.weights = weights\n",
        "        \n",
        "        #when loss stops decresing, decrease learning rate\n",
        "        self.reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                           factor=0.5,\n",
        "                                           patience=2,\n",
        "                                           min_lr=1e-8,\n",
        "                                           mode=\"min\")\n",
        "        \n",
        "\n",
        "        self.e_stopping = EarlyStopping(monitor=\"val_loss\",\n",
        "                                        min_delta=0.01,\n",
        "                                        patience=5,\n",
        "                                        mode=\"min\",\n",
        "                                        restore_best_weights=True)\n",
        "        \n",
        "    def build_model(self):\n",
        "        transfer = self.engine(include_top=False, weights=self.weights, input_shape=self.input_dims,\n",
        "                             backend = keras.backend, layers = keras.layers,\n",
        "                             models = keras.models, utils = keras.utils)\n",
        "        x = transfer.output\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "        x = Dense(100, activation=\"relu\")(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "        any_logits = Dense(1, kernel_initializer=he_normal(seed=11))(x)\n",
        "        any_pred = Activation(\"sigmoid\", name=\"any_predictions\")(any_logits)\n",
        "        x = concatenate([any_pred, x])\n",
        "        sub_pred = Dense(self.num_classes,\n",
        "                         name=\"subtype_pred\",\n",
        "                         kernel_initializer=he_normal(seed=12),\n",
        "                         activation=\"sigmoid\")(x) \n",
        "        self.model = Model(inputs=transfer.input, outputs=[any_pred, sub_pred])\n",
        "    \n",
        "    def compile_model(self):\n",
        "      self.model.compile(optimizer=Adam(self.learning_rate),\n",
        "                         loss=['binary_crossentropy', 'categorical_crossentropy'],\n",
        "                         loss_weights = [0.5, 0.5],\n",
        "                         metrics=self.metrics_list)\n",
        "    \n",
        "    def learn(self):\n",
        "        return self.model.fit_generator(generator=self.train_generator,\n",
        "                    validation_data=self.val_generator,\n",
        "                    epochs=self.epochs,\n",
        "                    callbacks=[self.reduce_lr, self.e_stopping],\n",
        "                    use_multiprocessing=False,\n",
        "                    workers=CORES)\n",
        "    \n",
        "    def load_weights(self, path):\n",
        "        self.model.load_weights(path)\n",
        "    \n",
        "    def save_weights(self, path):\n",
        "      self.model.save_weights(path)\n",
        "    \n",
        "    def to_json(self, path):\n",
        "      json = self.model.to_json()\n",
        "      with open(path, \"w\") as json_file:\n",
        "        json_file.write(json)\n",
        "    \n",
        "    def summary(self):\n",
        "      self.model.summary()\n",
        "    \n",
        "    def predict(self, data_generator):\n",
        "        predictions = self.model.predict_generator(data_generator, workers=CORES)\n",
        "        return predictions\n",
        "\n",
        "class Transfer_network_CH:\n",
        "    \n",
        "    def __init__(self, engine, loss_fun, metrics_list, input_dims, train_generator, val_generator,\n",
        "                 epochs, learning_rate=1e-3, num_classes=5, weights='imagenet'):\n",
        "        \n",
        "        self.engine = engine\n",
        "        self.loss_fun = loss_fun\n",
        "        self.metrics_list = metrics_list\n",
        "        self.input_dims = input_dims\n",
        "        self.train_generator = train_generator\n",
        "        self.val_generator = val_generator\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_classes = num_classes\n",
        "        self.weights = weights\n",
        "\n",
        "        #when loss stops decreasing, decrease learning rate\n",
        "        self.reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                           factor=0.5,\n",
        "                                           patience=2,\n",
        "                                           min_lr=1e-8,\n",
        "                                           mode=\"min\")\n",
        "        \n",
        "\n",
        "        self.e_stopping = EarlyStopping(monitor=\"val_loss\",\n",
        "                                        min_delta=0.01,\n",
        "                                        patience=5,\n",
        "                                        mode=\"min\",\n",
        "                                        restore_best_weights=True)\n",
        "        \n",
        "    def build_model(self):\n",
        "        transfer = self.engine(include_top=False, weights=self.weights, input_shape=self.input_dims,\n",
        "                             backend = keras.backend, layers = keras.layers,\n",
        "                             models = keras.models, utils = keras.utils)\n",
        "        x = transfer.output\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "        x = Dense(100, activation=\"relu\")(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "        any_logits = Dense(1, kernel_initializer=he_normal(seed=11))(x)\n",
        "        any_pred = Activation(\"sigmoid\", name=\"any_predictions\")(any_logits)\n",
        "        x = concatenate([any_pred, x])\n",
        "        sub_pred = Dense(self.num_classes,\n",
        "                         name=\"subtype_pred\",\n",
        "                         kernel_initializer=he_normal(seed=12),\n",
        "                         activation=\"sigmoid\")(x) \n",
        "        self.model = Model(inputs=transfer.input, outputs=[any_pred, sub_pred])\n",
        "    \n",
        "    def compile_model(self):\n",
        "      self.model.compile(optimizer=Adam(self.learning_rate),\n",
        "                         loss=['binary_crossentropy', 'categorical_crossentropy'],\n",
        "                         loss_weights = [0.8, 0.1],\n",
        "                         metrics=self.metrics_list)\n",
        "    \n",
        "    def learn(self):\n",
        "        return self.model.fit_generator(generator=self.train_generator,\n",
        "                    validation_data=self.val_generator,\n",
        "                    epochs=self.epochs,\n",
        "                    callbacks=[self.reduce_lr, self.e_stopping],\n",
        "                    use_multiprocessing=False,\n",
        "                    workers=CORES)\n",
        "    \n",
        "    def load_weights(self, path):\n",
        "        self.model.load_weights(path)\n",
        "    \n",
        "    def save_weights(self, path):\n",
        "      self.model.save_weights(path)\n",
        "    \n",
        "    def to_json(self, path):\n",
        "      json = self.model.to_json()\n",
        "      with open(path, \"w\") as json_file:\n",
        "        json_file.write(json)\n",
        "    \n",
        "    def summary(self):\n",
        "      self.model.summary()\n",
        "    \n",
        "    def predict(self, data_generator):\n",
        "        predictions = self.model.predict_generator(data_generator, workers=CORES)\n",
        "        return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKT-ULVD8BXS",
        "colab_type": "text"
      },
      "source": [
        "## Train-test split\n",
        "We'll save 10% of images for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvesRwH3esiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "group_kfold = GroupKFold(n_splits=10)\n",
        "\n",
        "for tr, ts in group_kfold.split(train, groups=train.Patient):\n",
        "  training = train.iloc[tr]\n",
        "  testing = train.iloc[ts]\n",
        "\n",
        "testing = testing.set_index('ID', drop=False)\n",
        "print(training.shape)\n",
        "print(testing.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnMyp7JLfY7A",
        "colab_type": "text"
      },
      "source": [
        "### Train-validation split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJKR7Aam8Di7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_val_folds = GroupKFold(n_splits=20)\n",
        "\n",
        "for tr, ts in tr_val_folds.split(training, groups=training.Patient):\n",
        "  train_data = training.iloc[tr]\n",
        "  val_data = training.iloc[ts]\n",
        "\n",
        "train_data = train_data.set_index('ID', drop=False)\n",
        "val_data = val_data.set_index('ID', drop=False)\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7ZU9CYw9h1g",
        "colab_type": "text"
      },
      "source": [
        "Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swizCnvL-59_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataloader_CH = DataGenerator_Channels(IDs=train_data.ID,\n",
        "                                 labels=train_data.loc[:,'any':'subdural'],\n",
        "                                 batch_size=30,\n",
        "                                 img_size=(200,200,3),\n",
        "                                 num_classes=5)\n",
        "val_dataloader_CH = DataGenerator_Channels(IDs=val_data.ID,\n",
        "                               labels=val_data.loc[:,'any':'subdural'],\n",
        "                               batch_size=5,\n",
        "                               img_size=(200,200,3),\n",
        "                               num_classes=5)\n",
        "\n",
        "train_dataloader_F = DataGenerator_Full(IDs=train_data.ID,\n",
        "                                 labels=train_data.loc[:,'any':'subdural'],\n",
        "                                 batch_size=30,\n",
        "                                 img_size=(200,200,3),\n",
        "                                 num_classes=5)\n",
        "val_dataloader_F = DataGenerator_Full(IDs=val_data.ID,\n",
        "                               labels=val_data.loc[:,'any':'subdural'],\n",
        "                               batch_size=5,\n",
        "                               img_size=(200,200,3),\n",
        "                               num_classes=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3gZYtM-RClm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sanity check of datagenerator\n",
        "%time X, [y1, y2] = train_dataloader_CH.__getitem__(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6ek2caERIvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X.shape)\n",
        "print(y1.shape)\n",
        "print(y2.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7V55DaYnDwP",
        "colab_type": "text"
      },
      "source": [
        "#### Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFyK9ouInF9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZWjrFD59jH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_CH = Transfer_network_CH(engine=InceptionV3, loss_fun=None,\n",
        "                         metrics_list={\"any_predictions\":\"binary_accuracy\",\n",
        "                                       \"subtype_pred\": \"categorical_accuracy\"},\n",
        "                         input_dims=(200,200,3),\n",
        "                         train_generator=train_dataloader_CH,\n",
        "                         val_generator=val_dataloader_CH,\n",
        "                         epochs=EPOCHS,\n",
        "                         learning_rate=1e-3) \n",
        "model_CH.build_model()\n",
        "model_CH.compile_model()\n",
        "history_CH = model_CH.learn()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MLy10WEoVEQ",
        "colab_type": "text"
      },
      "source": [
        "Save history to csv for later and also the model architecture and learned weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "798qWVoGmQym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "channels_history = pd.DataFrame.from_dict(history_CH.history)\n",
        "channels_history['epoch']=history_CH.epoch\n",
        "channels_history = channels_history.set_index('epoch')\n",
        "channels_history.head(5)\n",
        "\n",
        "#save history to csv\n",
        "channels_history.to_csv(RESULT_PATH + 'history_transfer_channels.csv')\n",
        "\n",
        "#save architecture\n",
        "model_CH.to_json(path=\"transfer_channels.json\")\n",
        "\n",
        "#save weights\n",
        "model_CH.save_weights(MODEL_PATH + \"transfer_channels.h5\")\n",
        "\n",
        "with open(MODEL_PATH + 'transfer_channels_summary.txt', 'w') as f:\n",
        "    with redirect_stdout(f):\n",
        "        model_CH.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFkdNar3yfYt",
        "colab_type": "text"
      },
      "source": [
        "Predict on testing data and save predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WivALRVyeNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataloader_CH = DataGenerator_Channels(IDs=testing.ID,\n",
        "                               labels=testing.loc[:,'any':'subdural'],\n",
        "                               batch_size=5,\n",
        "                               img_size=(200,200,3),\n",
        "                               num_classes=5)\n",
        "CH_test_pred_any, CH_test_pred_sub = model_CH.predict(test_dataloader_CH)\n",
        "any_pred = pd.DataFrame(CH_test_pred_any)\n",
        "sub_pred = pd.DataFrame(CH_test_pred_sub)\n",
        "ID = pd.DataFrame(testing.ID.values)\n",
        "channels_pred = pd.concat([ID,any_pred, sub_pred], axis=1)\n",
        "channels_pred.columns = [\"ID\", \"any\", 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
        "channels_pred.to_csv(RESULT_PATH + \"transfer_channels_preds.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93mbUJgYzPRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "fig, ax = plt.subplots(2,1, figsize=(20,10))\n",
        "sns.distplot(CH_test_pred_any[:,0], ax=ax[0], color=\"Purple\")\n",
        "sns.distplot(CH_test_pred_sub[:,0], ax=ax[1])\n",
        "sns.distplot(CH_test_pred_sub[:,1], ax=ax[1])\n",
        "sns.distplot(CH_test_pred_sub[:,2], ax=ax[1])\n",
        "sns.distplot(CH_test_pred_sub[:,3], ax=ax[1])\n",
        "sns.distplot(CH_test_pred_sub[:,4], ax=ax[1])\n",
        "ax[0].set_title(\"Predicted probability of hemorrhage occurence in dev batch\")\n",
        "ax[1].set_title(\"Predicted probability of hemorrhage subtypes in dev batch\")\n",
        "fig.savefig(RESULT_PATH + 'Transfer_channels_dist.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X780p5lADOMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_F = Transfer_network_F(engine=InceptionV3, loss_fun=None,\n",
        "                         metrics_list={\"any_predictions\":\"binary_accuracy\",\n",
        "                                       \"subtype_pred\": \"categorical_accuracy\"},\n",
        "                         input_dims=(200,200,3),\n",
        "                         train_generator=train_dataloader_F,\n",
        "                         val_generator=val_dataloader_F,\n",
        "                         epochs=EPOCHS,\n",
        "                         learning_rate=1e-3) \n",
        "model_F.build_model()\n",
        "model_F.compile_model()\n",
        "history_F = model_F.learn()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ht2cpeFoZ6f",
        "colab_type": "text"
      },
      "source": [
        "Save history for later and also the model architecture and learned weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POgMB8aXocCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_history = pd.DataFrame.from_dict(history_F.history)\n",
        "full_history['epoch']=history_F.epoch\n",
        "full_history = full_history.set_index('epoch')\n",
        "full_history.head(5)\n",
        "\n",
        "#save history to csv\n",
        "full_history.to_csv(RESULT_PATH + 'history_transfer_full.csv')\n",
        "\n",
        "#save architecture\n",
        "model_F.to_json(path=\"transfer_full.json\")\n",
        "\n",
        "#save weights\n",
        "model_F.save_weights(MODEL_PATH + \"transfer_full.h5\")\n",
        "\n",
        "with open(MODEL_PATH + 'transfer_full_summary.txt', 'w') as f:\n",
        "    with redirect_stdout(f):\n",
        "        model_F.summary()\n",
        "\n",
        "test_dataloader_F = DataGenerator_Full(IDs=testing.ID,\n",
        "                               labels=testing.loc[:,'any':'subdural'],\n",
        "                               batch_size=5,\n",
        "                               img_size=(200,200,3),\n",
        "                               num_classes=5)\n",
        "F_test_pred_any, F_test_pred_sub = model_F.predict(test_dataloader_F)\n",
        "any_pred = pd.DataFrame(F_test_pred_any)\n",
        "sub_pred = pd.DataFrame(F_test_pred_sub)\n",
        "ID = pd.DataFrame(testing.ID.values)\n",
        "full_pred = pd.concat([ID,any_pred, sub_pred], axis=1)\n",
        "full_pred.columns = [\"ID\", \"any\", 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
        "full_pred.to_csv(RESULT_PATH + \"transfer_full_preds.csv\")\n",
        "\n",
        "fig, ax = plt.subplots(2,1, figsize=(20,10))\n",
        "sns.distplot(F_test_pred_any[:,0], ax=ax[0], color=\"Purple\")\n",
        "sns.distplot(F_test_pred_sub[:,0], ax=ax[1])\n",
        "sns.distplot(F_test_pred_sub[:,1], ax=ax[1])\n",
        "sns.distplot(F_test_pred_sub[:,2], ax=ax[1])\n",
        "sns.distplot(F_test_pred_sub[:,3], ax=ax[1])\n",
        "sns.distplot(F_test_pred_sub[:,4], ax=ax[1])\n",
        "ax[0].set_title(\"Predicted probability of hemorrhage occurence in dev batch\")\n",
        "ax[1].set_title(\"Predicted probability of hemorrhage subtypes in dev batch\")\n",
        "fig.savefig(RESULT_PATH + 'Transfer_full_dist.png')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}